# TODO - Work in progress
# Configuration for evaluating AI model performance using a list of prompts.
# This file serves as a development roadmap for adding more advanced evaluation features.

evaluation_config:
  name: "Model Evaluation Suite" # Name for this specific evaluation run.
  
  # Global AI Provider and Model settings for the evaluation target.
  # These can be overridden at a more granular level if needed for specific tests.
  provider: "openai" # Example: "openai", "xai", "google"
  model: "gpt-4o"    # Example: "gpt-4o", "grok-3", "gemini-1.5-flash"

  # Output and Logging Configuration
  log_directory: "./evaluation_logs/default_run" # Directory to store evaluation results and logs.
  # TODO: Add options for output formats (e.g., JSON, CSV) and detail level.

  # Prompt Handling Modes
  # Defines how the list of prompts will be processed.
  # Options:
  #   - "one_shot": Each prompt is treated as an independent request.
  #                 The model responds to one prompt, and the interaction ends.
  #   - "blind_multi_turn": Prompts are fed sequentially as a continuous conversation.
  #                         The model maintains conversational context across prompts.
  #                         "Blind" means no external feedback/judgement within the turn sequence.
  prompt_processing_mode: "one_shot" # Current mode: single independent prompts.

  # List of prompts to be used for evaluation.
  # Each item in the list represents a distinct input to the model.
  prompts:
    - "What is the capital of France?"
    - "Explain the concept of quantum entanglement in simple terms."
    - "Write a short, humorous limerick about a cat."
    - "If a tree falls in a forest and no one is around to hear it, does it make a sound?"
    - "Translate 'Hello, how are you?' into Spanish."
    - "Describe the main plot points of the novel '1984'."
    - "What is the square root of 144?"

  # --- Development Roadmap / Future Features ---

  # LLM Judge Configuration (Future Enhancement)
  # This section will be uncommented and configured when the LLM judging capability is added.
  # judge_config:
  #   enabled: false # Set to true to enable LLM judging.
  #   judge_provider: "google" # Provider for the judging LLM.
  #   judge_model: "gemini-1.5-pro" # Model for the judging LLM.
  #   judge_system_prompt: |
  #     You are an impartial AI judge evaluating the quality, relevance, and accuracy of AI model responses.
  #     Your task is to compare the given prompt with the model's response and provide a score and a brief explanation.
  #     Score on a scale of 1 to 5, where 1 is very poor and 5 is excellent.
  #   # Criteria for judging, could be more elaborate.
  #   judge_criteria:
  #     - "Accuracy"
  #     - "Coherence"
  #     - "Relevance"
  #     - "Conciseness"
  #   # Output format for the judge's verdict.
  #   judge_output_format: |
  #     Score: [1-5]
  #     Reasoning: [Brief explanation]

  # Expected Outputs (Future Enhancement)
  # For automated testing against known correct answers.
  # expected_outputs:
  #   - prompt: "What is the capital of France?"
  #     expected: "Paris"
  #     tolerance: "exact" # or "semantic"

  # Performance Metrics (Future Enhancement)
  # To track response times, token usage, etc.
  # metrics:
  #   track_tokens: true
  #   track_latency: true

  # Batch Processing (Future Enhancement)
  # For handling a large number of prompts more efficiently.
  # batch_size: 10 # Number of prompts to process concurrently.

  # Error Handling & Retries (Future Enhancement)
  # retry_attempts: 3
  # retry_delay_seconds: 5

  # Reporting Features (Future Enhancement)
  # report_generation: true
  # report_template: "detailed_html_report.jinja"
  